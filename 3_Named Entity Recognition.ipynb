{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition or NER for short is a natural language processing task used to identify important named entities in the text such as people, places and organizations. They can even be dates, states, works of art and other categories depending on the libraries and notation you use. \n",
    "\n",
    "NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who? what? when and where?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is the process of identifying and classifying entities such as persons, locations and organisations in the full-text in order to enhance searchability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kind of problems can NER solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a sub-task of natural language processing (NLP) that involves identifying and classifying named entities in a text. NER is used to extract structured information from unstructured text data and is commonly used in a wide range of applications to solve various types of problems. Some examples of problems that NER can help solve include:\n",
    "\n",
    "- Information extraction: NER can be used to extract specific pieces of information from a text, such as names, dates, and locations, and organize them in a structured format. This can be useful for tasks such as building databases or creating summaries of text documents.\n",
    "\n",
    "- Question answering: NER can be used to identify named entities in a question and use them to search for relevant information in a database or text corpus.\n",
    "\n",
    "- Text summarization: NER can be used to extract key named entities from a text and use them to generate a concise summary of the main points of the text.\n",
    "\n",
    "- Entity disambiguation: NER can be used to identify and disambiguate named entities that may have multiple meanings or refer to different entities in different contexts.\n",
    "\n",
    "- Text classification: NER can be used as a feature in text classification tasks, such as identifying the topic or genre of a text.\n",
    "\n",
    "Overall, NER can be used to extract structured information from unstructured text data and solve a variety of problems in natural language processing and other related fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example of Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/brindhamanivannan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/brindhamanivannan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama was born in Hawaii. He was the 44th President of the United States.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack',\n",
       " 'Obama',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Hawaii',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'the',\n",
       " '44th',\n",
       " 'President',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/brindhamanivannan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Barack', 'NNP'),\n",
       " ('Obama', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Hawaii', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('44th', 'JJ'),\n",
       " ('President', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_tokens))\n",
    "print(type(tagged_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Barack/NNP)\n",
      "(PERSON Obama/NNP)\n",
      "(GPE Hawaii/NNP)\n",
      "(GPE United/NNP States/NNPS)\n"
     ]
    }
   ],
   "source": [
    "# Perform named entity recognition\n",
    "\n",
    "entities = [chunk for chunk in nltk.ne_chunk(tagged_tokens) if isinstance(chunk, nltk.Tree)]\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NER code for information extraction - full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Barack/NNP)\n",
      "(PERSON Obama/NNP)\n",
      "(GPE Hawaii/NNP)\n",
      "(GPE United/NNP States/NNPS)\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Tag the tokens\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Perform named entity recognition \n",
    "entities = [chunk for chunk in nltk.ne_chunk(tagged_tokens) if isinstance(chunk, nltk.Tree)]\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is using the Natural Language Toolkit (nltk) library to perform Named Entity Recognition (NER) on a given piece of text. Here is a brief explanation of each line of code:\n",
    "\n",
    "- The first line defines a string called text that contains the input text for the NER process.\n",
    "- The second line tokenizes the text string into a list of individual words, called tokens.\n",
    "- The third line tags each token with a part-of-speech (POS) tag, which indicates the word's grammatical role in the sentence. The result is a list of tuples called tagged_tokens, where each tuple consists of a word and its POS tag.\n",
    "- The fourth line uses nltk.ne_chunk() to identify named entities in the tagged_tokens list. The named entities are returned as a tree-like structure, where the leaves are individual words and the branches are the named entities that they belong to. The chunk variable in the list comprehension will iterate over each element of this tree-like structure. The isinstance(chunk, nltk.Tree) expression checks whether each element is a named entity (a tree) or a single word (a leaf). If it is a named entity, it is added to the entities list.\n",
    "- The final for loop iterates over each named entity in the entities list and prints it to the console.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another simple example of Named Entity Recognition (NER) code that extracts information from a given piece of text using the Natural Language Toolkit (nltk) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: Narendra\n",
      "PERSON: Modi\n",
      "GPE: Vadnagar\n",
      "GPE: India\n",
      "GPE: India\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Tag each token with its part-of-speech (POS)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Use nltk's named entity chunker to extract named entities\n",
    "    entities = nltk.ne_chunk(tagged_tokens)\n",
    "    \n",
    "    # Iterate over the entities and extract the information we want\n",
    "    for entity in entities:\n",
    "        # Check if the entity is a named entity\n",
    "        if isinstance(entity, nltk.Tree):\n",
    "            # Get the label for the entity (e.g. \"PERSON\")\n",
    "            label = entity.label()\n",
    "            # Get the string representation of the entity\n",
    "            entity_string = \" \".join([word for word, tag in entity])\n",
    "            # Print the entity information\n",
    "            print(f\"{label}: {entity_string}\")\n",
    "\n",
    "# Test the extract_entities() function\n",
    "text = \"Narendra Modi was born in Vadnagar, India. He is the 14th and current Prime Minister of India since 2014.\"\n",
    "extract_entities(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called extract_entities() that takes a string of text as input and uses the nltk library to identify named entities in the text. It then extracts and prints the label (e.g. \"PERSON\") and string representation of each named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: Barack\n",
      "PERSON: Obama\n",
      "GPE: Hawaii\n",
      "GPE: United States\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "extract_entities(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: Narendra Modi\n",
      "GPE: Indian\n",
      "GPE: India\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Prime Minister Narendra Modi’s government is pushing to overhaul the country’s heavily-regulated education sector to enable Indian students to obtain foreign qualifications at an affordable cost and make India an attractive global study destination. The move will also help overseas institutions to tap the nation’s young population.\"\n",
    "extract_entities(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE: India\n",
      "ORGANIZATION: Microsoft\n",
      "ORGANIZATION: Alphabet Inc.\n",
      "ORGANIZATION: Global\n",
      "ORGANIZATION: Competitiveness Index\n",
      "GPE: Indian\n",
      "GPE: India\n",
      "ORGANIZATION: University Grants Commission\n"
     ]
    }
   ],
   "source": [
    "text_3 = \"Even as India’s universities and colleges have produced chief executive officers at companies from Microsoft Corp. to Alphabet Inc., many fare poorly in global rankings. The country needs to boost its education sector to become more competitive and close the growing gap between college curricula and market demand. It’s currently ranked 101 among 133 nations in the Global Talent Competitiveness Index of 2022 that measures a nation’s ability to grow, attract and retain talent.Some universities have already set up partnerships with Indian institutions, allowing students to partially study in India and complete their degrees on the main campus abroad. The current move will encourage these overseas institutions to set up campuses without local partners. The University Grants Commission’s final draft will be presented to the parliament for its approval before becoming law.\"\n",
    "\n",
    "extract_entities(text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NER code for question answering - full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narendra was born in Vadnagar\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def answer_question(question, text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Tag each token with its part-of-speech (POS)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Use nltk's named entity chunker to extract named entities\n",
    "    entities = nltk.ne_chunk(tagged_tokens)\n",
    "    \n",
    "    # Initialize variables to store the person and location\n",
    "    person = None\n",
    "    location = None\n",
    "    \n",
    "    # Iterate over the entities and extract the information we want\n",
    "    for entity in entities:\n",
    "        # Check if the entity is a named entity\n",
    "        if isinstance(entity, nltk.Tree):\n",
    "            # Get the label for the entity (e.g. \"PERSON\")\n",
    "            label = entity.label()\n",
    "            # Get the string representation of the entity\n",
    "            entity_string = \" \".join([word for word, tag in entity])\n",
    "            \n",
    "            # Store the person and location\n",
    "            if label == \"PERSON\" and person is None:\n",
    "                person = entity_string\n",
    "            elif label == \"GPE\" and location is None:\n",
    "                location = entity_string\n",
    "    \n",
    "    # Extract the answer to the question\n",
    "    if \"born\" in question.lower():\n",
    "        print(f\"{person} was born in {location}\")\n",
    "    elif \"president\" in question.lower():\n",
    "        print(f\"{person} was the president\")\n",
    "\n",
    "# Test the answer_question() function\n",
    "new_text = \"Narendra Modi was born in Vadnagar, India. He is the 14th and current Prime Minister of India since 2014.\"\n",
    "new_question = \"Where was Narendra Modi born?\"\n",
    "\n",
    "answer_question(new_question, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack was born in Hawaii\n"
     ]
    }
   ],
   "source": [
    "# Test the answer_question() function\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "question = \"Where was Barack Obama born?\"\n",
    "answer_question(question, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manivannan was born in Madurai\n"
     ]
    }
   ],
   "source": [
    "# Test the answer_question() function\n",
    "new_text_1 = \"Manivannan was born in Madurai.\"\n",
    "new_question_1 = \"Where was Manivannan born?\"\n",
    "answer_question(new_question_1, new_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NER code for Text summarization - full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama Hawaii United States\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def summarize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Tag each token with its part-of-speech (POS)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Use nltk's named entity chunker to extract named entities\n",
    "    entities = nltk.ne_chunk(tagged_tokens)\n",
    "    \n",
    "    # Initialize a list to store the named entities\n",
    "    named_entities = []\n",
    "    \n",
    "    # Iterate over the entities and extract the named entities\n",
    "    for entity in entities:\n",
    "        # Check if the entity is a named entity\n",
    "        if isinstance(entity, nltk.Tree):\n",
    "            # Get the label for the entity (e.g. \"PERSON\")\n",
    "            label = entity.label()\n",
    "            # Get the string representation of the entity\n",
    "            entity_string = \" \".join([word for word, tag in entity])\n",
    "            \n",
    "            # Add the entity to the list of named entities\n",
    "            named_entities.append(entity_string)\n",
    "    \n",
    "    # Join the named entities into a single string\n",
    "    summary = \" \".join(named_entities)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test the summarize() function\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "summary = summarize(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called summarize() that takes a string of text as input and returns a summary of the text as a string. The summary is generated by extracting the named entities from the text using the nltk library and then joining them into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narendra Modi Vadnagar India India\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize() function\n",
    "text_mod = \"Narendra Modi was born in Vadnagar, India. He is the 14th and current Prime Minister of India since 2014.\"\n",
    "summary = summarize(text_mod)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NER code for Entity disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Entity disambiguation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity disambiguation is the process of identifying the correct meaning or interpretation of a named entity in a text. A named entity is a real-world object with a proper name, such as a person, organization, or location.\n",
    "\n",
    "For example, consider the following sentence: \"Barack Obama was the President of the United States from 2009 to 2017.\" In this sentence, \"Barack Obama\" is a named entity that refers to a specific person, while \"United States\" is a named entity that refers to a specific location. However, there may be multiple people or locations with the same name, so it's important to disambiguate the entities in order to correctly understand the meaning of the text.\n",
    "\n",
    "Entity disambiguation is an important task in natural language processing and information retrieval, as it helps to correctly interpret the meaning of a text and identify the relevant information. There are various approaches to entity disambiguation, including using external knowledge sources such as Wikipedia or using machine learning techniques to predict the correct meaning of an entity based on its context in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "https://en.wikipedia.org/wiki/Barack_Obama\n",
      "the United States GPE\n",
      "2009 to 2017 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import wikipedia\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define a function to disambiguate named entities\n",
    "def disambiguate_entities(text):\n",
    "    # Process the text with the model\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Iterate over the named entities\n",
    "    for ent in doc.ents:\n",
    "        # Print the entity text and label\n",
    "        print(ent.text, ent.label_)\n",
    "        \n",
    "        # If the entity is a person, get the Wikipedia page\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            wikipedia_page = wikipedia.page(ent.text)\n",
    "            print(wikipedia_page.url)\n",
    "            \n",
    "# Disambiguate the entities in a sample text\n",
    "text = \"Barack Obama was the President of the United States from 2009 to 2017.\"\n",
    "disambiguate_entities(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple code processes the input text with the spacy model, which will identify and label the named entities in the text. It then iterates over the named entities and checks if they are labeled as \"PERSON\". If they are, it uses the wikipedia library to get the Wikipedia page for the person and prints the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NER code for Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the process of assigning predefined categories or labels to a piece of text. It is a common task in natural language processing, and it is useful for a wide range of applications such as sentiment analysis, spam filtering, and topic classification.\n",
    "\n",
    "For example, given a dataset of customer reviews for a product, a text classification model might be trained to predict whether a given review is positive or negative. Similarly, a text classification model might be trained to classify news articles into different categories such as sports, politics, or entertainment.\n",
    "\n",
    "Text classification typically involves preprocessing the text data and converting it into a numerical form that can be used by a machine learning model. Common techniques for preprocessing text data include tokenization, stemming, and removing stop words. Machine learning algorithms such as support vector machines, decision trees, and Naive Bayes can then be used to train a model on the preprocessed data. The trained model can then be used to predict the class label for new, unseen text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Text Classification Datasets\n",
    "\n",
    "https://www.kaggle.com/datasets?search=text+classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of text classification using the sklearn library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "accuracy = classifier.score(X_test_vectors, y_test)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will load a dataset from a CSV file, split it into features (the text data) and labels (the class labels), and then split it into training and testing sets. It will then vectorize the text data using the CountVectorizer class, which converts the text into numerical data that can be used by a machine learning model. Finally, it will train a classifier using a Naive Bayes model, and evaluate the classifier on the test data by printing the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
