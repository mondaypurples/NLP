{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "Regular expressions are a powerful tool for working with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions (also known as \"regex\" or \"regexp\") are a way to describe patterns in text. They are commonly used in programming and text processing to search, match, and manipulate strings (sequences of characters).\n",
    "\n",
    "A regular expression is a sequence of characters that defines a search pattern. The search pattern can be used to match (and sometimes replace) strings, or to perform some other manipulation of strings.\n",
    "\n",
    "Regular expressions are composed of characters and special symbols. Some common characters and symbols used in regular expressions include:\n",
    "\n",
    "- .: matches any single character (except a newline)\n",
    "- *: matches zero or more of the preceding character or expression\n",
    "- +: matches one or more of the preceding character or expression\n",
    "- ?: matches zero or one of the preceding character or expression\n",
    "- [ ]: matches any character within the brackets\n",
    "- ^: matches the start of a line\n",
    "- $: matches the end of a line\n",
    "- \\d: matches any digit (equivalent to [0-9])\n",
    "- \\w: matches any word character (alphanumeric character plus underscore)\n",
    "\n",
    "There are many more characters and symbols that can be used in regular expressions, and the specific syntax and usage of regular expressions can vary depending on the programming language or tool being used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more examples of how the * character can be used in regular expressions:\n",
    "\n",
    "- a*: This regular expression would match zero or more a characters. For example, it would match the strings \"\", \"a\", \"aa\", \"aaa\", and so on.\n",
    "\n",
    "- [abc]*: This regular expression would match zero or more characters from the set {a, b, c}. For example, it would match the strings \"\", \"a\", \"ab\", \"abc\", \"b\", \"bc\", and so on.\n",
    "\n",
    "- .*: This regular expression would match zero or more of any character. It is often used as a \"wildcard\" to match any string. For example, .* would match any string, including the empty string.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression a+ would match one or more occurrences of the letter \"a\" in a string. It would match a, aa, aaa, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more examples of how the ? character can be used in regular expressions:\n",
    "\n",
    "- a?: This regular expression would match zero or one a characters. For example, it would match the strings \"\" and \"a\".\n",
    "\n",
    "- [abc]?: This regular expression would match zero or one characters from the set {a, b, c}. For example, it would match the strings \"\", \"a\", \"b\", and \"c\".\n",
    "\n",
    "- colou?r: This regular expression would match the words \"color\" or \"colour\". The ? after the u makes the preceding u optional, so the regex will match either \"color\" or \"colour\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # import re library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcdef') # first argument - pattern, second argument - string; returns a match object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex = '\\w+' # matches a word\n",
    "re.match(word_regex, 'hi there!') # matches the first word it finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.') # \\s space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. \n",
    "\n",
    "Else, you may encounter problems to do with escape sequences in strings. \n",
    "\n",
    "For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" (which means that it should be interpreted literally and any special characters within the string should be treated as plain text, rather than as escape sequences) - that is, the backslash will be treated as a literal character rather than an escape character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(r\"\\s+\", my_string) # find all spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(r\"\\w+\", my_string) # matches a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 't', 's', 'w', 'r', 'i', 't', 'e', 'e', 'g', 'x']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(r\"[a-z]\", my_string) # lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L', 'e', 't', 's', 'w', 'r', 'i', 't', 'e', 'R', 'e', 'g', 'E', 'x']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "re.findall(r\"\\w\", my_string) # match any word character - a letter or digit or an _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\n"
     ]
    }
   ],
   "source": [
    "my_string_1 = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "print(my_string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
     ]
    }
   ],
   "source": [
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string_1)) # (pattern, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('cd', 'abcde') # observe there is no result. Why? This is because match will try and match a string from the beginning until it cannot match any longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search versus Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we use search and match with the same pattern and string with the pattern is at the beginning of the string, we see we find identical matches. That is the case with matching and searching abcde with the pattern abc. \n",
    "\n",
    "- When we use search for a pattern that appears later in the string we get a result, but we don't get the same result using match. This is because match will try and match a string from the beginning until it cannot match any longer. Search will go through the ENTIRE string to look for match options. \n",
    "\n",
    "- If you need to find a pattern that might not be at the beginning of the string, you should use search. \n",
    "- If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a stream of text into individual words, phrases, symbols, or other meaningful elements, known as tokens. It is a common task in natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of tokenization involves:\n",
    "\n",
    "- Splitting the input text into individual words or symbols.\n",
    "- Filtering out irrelevant characters, such as punctuation and whitespace.\n",
    "- Normalizing the text, which may involve converting all words to lowercase, stemming (reducing words to their base form), or lemmatizing (converting words to their dictionary form).\n",
    "\n",
    "Tokenization is an important step in NLP because it allows the system to work with individual pieces of text, rather than large blocks of text. This makes it easier to analyze and manipulate the text, and enables the system to perform tasks such as language translation, text classification, and text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk library\n",
    "\n",
    "Natural Language Toolkit\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a library in Python that provides tools to work with human language data (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brindhamanivannan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hi there!\") # punctuations are individual tokens as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization examples\n",
    "\n",
    "- Breaking out words or sentences\n",
    "- Seperating punctuation\n",
    "- Seperating all hastags in a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/brindhamanivannan/Desktop/KaggleX/DataCamp/scene_one.txt\", \"r\") as file:\n",
    "    scene_one = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
      "SOLDIER #1: What?  Ridden on a horse?\n",
      "ARTHUR: Yes!\n",
      "SOLDIER #1: You're using coconuts!\n",
      "ARTHUR: What?\n",
      "SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n",
      "ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\n",
      "SOLDIER #1: Where'd you get the coconuts?\n",
      "ARTHUR: We found them.\n",
      "SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\n",
      "ARTHUR: What do you mean?\n",
      "SOLDIER #1: Well, this is a temperate zone.\n",
      "ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\n",
      "SOLDIER #1: Are you suggesting coconuts migrate?\n",
      "ARTHUR: Not at all.  They could be carried.\n",
      "SOLDIER #1: What?  A swallow carrying a coconut?\n",
      "ARTHUR: It could grip it by the husk!\n",
      "SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\n",
      "ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\n",
      "SOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\n",
      "ARTHUR: Please!\n",
      "SOLDIER #1: Am I right?\n",
      "ARTHUR: I'm not interested!\n",
      "SOLDIER #2: It could be carried by an African swallow!\n",
      "SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\n",
      "SOLDIER #2: Oh, yeah, I agree with that.\n",
      "ARTHUR: Will you ask your master if he wants to join my court at Camelot?!\n",
      "SOLDIER #1: But then of course a-- African swallows are non-migratory.\n",
      "SOLDIER #2: Oh, yeah...\n",
      "SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \n",
      "SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\n",
      "SOLDIER #1: No, they'd have to have it on a line.\n",
      "SOLDIER #2: Well, simple!  They'd just use a strand of creeper!\n",
      "SOLDIER #1: What, held under the dorsal guiding feathers?\n",
      "SOLDIER #2: Well, why not?\n",
      "print(scene_one)\n",
      "print(type(scene_one))\n",
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
      "SOLDIER #1: What?  Ridden on a horse?\n",
      "ARTHUR: Yes!\n",
      "SOLDIER #1: You're using coconuts!\n",
      "ARTHUR: What?\n",
      "SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n",
      "ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\n",
      "SOLDIER #1: Where'd you get the coconuts?\n",
      "ARTHUR: We found them.\n",
      "SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\n",
      "ARTHUR: What do you mean?\n",
      "SOLDIER #1: Well, this is a temperate zone.\n",
      "ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\n",
      "SOLDIER #1: Are you suggesting coconuts migrate?\n",
      "ARTHUR: Not at all.  They could be carried.\n",
      "SOLDIER #1: What?  A swallow carrying a coconut?\n",
      "ARTHUR: It could grip it by the husk!\n",
      "SOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\n",
      "ARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\n",
      "SOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\n",
      "ARTHUR: Please!\n",
      "SOLDIER #1: Am I right?\n",
      "ARTHUR: I'm not interested!\n",
      "SOLDIER #2: It could be carried by an African swallow!\n",
      "SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\n",
      "SOLDIER #2: Oh, yeah, I agree with that.\n",
      "ARTHUR: Will you ask your master if he wants to join my court at Camelot?!\n",
      "SOLDIER #1: But then of course a-- African swallows are non-migratory.\n",
      "SOLDIER #2: Oh, yeah...\n",
      "SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \n",
      "SOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\n",
      "SOLDIER #1: No, they'd have to have it on a line.\n",
      "SOLDIER #2: Well, simple!  They'd just use a strand of creeper!\n",
      "SOLDIER #1: What, held under the dorsal guiding feathers?\n",
      "SOLDIER #2: Well, why not?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize # sentences\n",
    "from nltk.tokenize import word_tokenize # words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!',\n",
       " '[clop clop clop] \\nSOLDIER #1: Halt!',\n",
       " 'Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.',\n",
       " 'King of the Britons, defeator of the Saxons, sovereign of all England!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[3] # 4th sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTHUR',\n",
       " ':',\n",
       " 'It',\n",
       " 'is',\n",
       " 'I',\n",
       " ',',\n",
       " 'Arthur',\n",
       " ',',\n",
       " 'son',\n",
       " 'of',\n",
       " 'Uther',\n",
       " 'Pendragon',\n",
       " ',',\n",
       " 'from',\n",
       " 'the',\n",
       " 'castle',\n",
       " 'of',\n",
       " 'Camelot',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE', '1', ':', '[', 'wind']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = word_tokenize(scene_one)\n",
    "unique_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '#',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " '1',\n",
       " '2',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'ARTHUR',\n",
       " 'African',\n",
       " 'Am',\n",
       " 'Are',\n",
       " 'Arthur',\n",
       " 'Britons',\n",
       " 'But',\n",
       " 'Camelot',\n",
       " 'Court',\n",
       " 'England',\n",
       " 'European',\n",
       " 'Found',\n",
       " 'Halt',\n",
       " 'I',\n",
       " 'In',\n",
       " 'It',\n",
       " 'KING',\n",
       " 'King',\n",
       " 'Listen',\n",
       " 'Mercea',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Oh',\n",
       " 'Patsy',\n",
       " 'Pendragon',\n",
       " 'Please',\n",
       " 'Pull',\n",
       " 'Ridden',\n",
       " 'SCENE',\n",
       " 'SOLDIER',\n",
       " 'Saxons',\n",
       " 'So',\n",
       " 'Supposing',\n",
       " 'That',\n",
       " 'The',\n",
       " 'They',\n",
       " 'Uther',\n",
       " 'Wait',\n",
       " 'We',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'Where',\n",
       " 'Who',\n",
       " 'Whoa',\n",
       " 'Will',\n",
       " 'Yes',\n",
       " 'You',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'agree',\n",
       " 'air-speed',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anyway',\n",
       " 'are',\n",
       " 'ask',\n",
       " 'at',\n",
       " 'back',\n",
       " 'bangin',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'bird',\n",
       " 'breadth',\n",
       " 'bring',\n",
       " 'but',\n",
       " 'by',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'castle',\n",
       " 'climes',\n",
       " 'clop',\n",
       " 'coconut',\n",
       " 'coconuts',\n",
       " 'could',\n",
       " 'course',\n",
       " 'court',\n",
       " 'covered',\n",
       " 'creeper',\n",
       " 'defeator',\n",
       " 'do',\n",
       " 'does',\n",
       " 'dorsal',\n",
       " 'empty',\n",
       " 'every',\n",
       " 'feathers',\n",
       " 'five',\n",
       " 'fly',\n",
       " 'forty-three',\n",
       " 'found',\n",
       " 'from',\n",
       " 'get',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'got',\n",
       " 'grip',\n",
       " 'grips',\n",
       " 'guiding',\n",
       " 'halves',\n",
       " 'have',\n",
       " 'he',\n",
       " 'held',\n",
       " 'here',\n",
       " 'horse',\n",
       " 'house',\n",
       " 'husk',\n",
       " 'if',\n",
       " 'in',\n",
       " 'interested',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'join',\n",
       " 'just',\n",
       " 'kingdom',\n",
       " 'knights',\n",
       " 'land',\n",
       " 'length',\n",
       " 'line',\n",
       " 'lord',\n",
       " 'maintain',\n",
       " 'martin',\n",
       " 'master',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'migrate',\n",
       " 'minute',\n",
       " 'must',\n",
       " 'my',\n",
       " \"n't\",\n",
       " 'needs',\n",
       " 'non-migratory',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'or',\n",
       " 'order',\n",
       " 'other',\n",
       " 'ounce',\n",
       " 'our',\n",
       " 'plover',\n",
       " 'point',\n",
       " 'pound',\n",
       " 'print',\n",
       " 'question',\n",
       " 'ratios',\n",
       " 'ridden',\n",
       " 'right',\n",
       " 'scene_one',\n",
       " 'search',\n",
       " 'second',\n",
       " 'seek',\n",
       " 'servant',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'snows',\n",
       " 'son',\n",
       " 'south',\n",
       " 'sovereign',\n",
       " 'speak',\n",
       " 'strand',\n",
       " 'strangers',\n",
       " 'suggesting',\n",
       " 'sun',\n",
       " 'swallow',\n",
       " 'swallows',\n",
       " 'tell',\n",
       " 'temperate',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'through',\n",
       " 'times',\n",
       " 'to',\n",
       " 'together',\n",
       " 'tropical',\n",
       " 'trusty',\n",
       " 'two',\n",
       " 'type',\n",
       " 'under',\n",
       " 'use',\n",
       " 'using',\n",
       " 'velocity',\n",
       " 'wants',\n",
       " 'warmer',\n",
       " 'weight',\n",
       " 'where',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'wind',\n",
       " 'wings',\n",
       " 'winter',\n",
       " 'with',\n",
       " 'yeah',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'zone'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '#',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'em\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " '1',\n",
       " '2',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'ARTHUR',\n",
       " 'African',\n",
       " 'Am',\n",
       " 'Are',\n",
       " 'Arthur',\n",
       " 'Britons',\n",
       " 'But',\n",
       " 'Camelot',\n",
       " 'Court',\n",
       " 'England',\n",
       " 'European',\n",
       " 'Found',\n",
       " 'Halt',\n",
       " 'I',\n",
       " 'In',\n",
       " 'It',\n",
       " 'KING',\n",
       " 'King',\n",
       " 'Listen',\n",
       " 'Mercea',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Oh',\n",
       " 'Patsy',\n",
       " 'Pendragon',\n",
       " 'Please',\n",
       " 'Pull',\n",
       " 'Ridden',\n",
       " 'SCENE',\n",
       " 'SOLDIER',\n",
       " 'Saxons',\n",
       " 'So',\n",
       " 'Supposing',\n",
       " 'That',\n",
       " 'The',\n",
       " 'They',\n",
       " 'Uther',\n",
       " 'Wait',\n",
       " 'We',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'Where',\n",
       " 'Who',\n",
       " 'Whoa',\n",
       " 'Will',\n",
       " 'Yes',\n",
       " 'You',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'agree',\n",
       " 'air-speed',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anyway',\n",
       " 'are',\n",
       " 'ask',\n",
       " 'at',\n",
       " 'back',\n",
       " 'bangin',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'bird',\n",
       " 'breadth',\n",
       " 'bring',\n",
       " 'but',\n",
       " 'by',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'castle',\n",
       " 'climes',\n",
       " 'clop',\n",
       " 'coconut',\n",
       " 'coconuts',\n",
       " 'could',\n",
       " 'course',\n",
       " 'court',\n",
       " 'covered',\n",
       " 'creeper',\n",
       " 'defeator',\n",
       " 'do',\n",
       " 'does',\n",
       " 'dorsal',\n",
       " 'empty',\n",
       " 'every',\n",
       " 'feathers',\n",
       " 'five',\n",
       " 'fly',\n",
       " 'forty-three',\n",
       " 'found',\n",
       " 'from',\n",
       " 'get',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'got',\n",
       " 'grip',\n",
       " 'grips',\n",
       " 'guiding',\n",
       " 'halves',\n",
       " 'have',\n",
       " 'he',\n",
       " 'held',\n",
       " 'here',\n",
       " 'horse',\n",
       " 'house',\n",
       " 'husk',\n",
       " 'if',\n",
       " 'in',\n",
       " 'interested',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'join',\n",
       " 'just',\n",
       " 'kingdom',\n",
       " 'knights',\n",
       " 'land',\n",
       " 'length',\n",
       " 'line',\n",
       " 'lord',\n",
       " 'maintain',\n",
       " 'martin',\n",
       " 'master',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'migrate',\n",
       " 'minute',\n",
       " 'must',\n",
       " 'my',\n",
       " \"n't\",\n",
       " 'needs',\n",
       " 'non-migratory',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'or',\n",
       " 'order',\n",
       " 'other',\n",
       " 'ounce',\n",
       " 'our',\n",
       " 'plover',\n",
       " 'point',\n",
       " 'pound',\n",
       " 'print',\n",
       " 'question',\n",
       " 'ratios',\n",
       " 'ridden',\n",
       " 'right',\n",
       " 'scene_one',\n",
       " 'search',\n",
       " 'second',\n",
       " 'seek',\n",
       " 'servant',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'snows',\n",
       " 'son',\n",
       " 'south',\n",
       " 'sovereign',\n",
       " 'speak',\n",
       " 'strand',\n",
       " 'strangers',\n",
       " 'suggesting',\n",
       " 'sun',\n",
       " 'swallow',\n",
       " 'swallows',\n",
       " 'tell',\n",
       " 'temperate',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'through',\n",
       " 'times',\n",
       " 'to',\n",
       " 'together',\n",
       " 'tropical',\n",
       " 'trusty',\n",
       " 'two',\n",
       " 'type',\n",
       " 'under',\n",
       " 'use',\n",
       " 'using',\n",
       " 'velocity',\n",
       " 'wants',\n",
       " 'warmer',\n",
       " 'weight',\n",
       " 'where',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'wind',\n",
       " 'wings',\n",
       " 'winter',\n",
       " 'with',\n",
       " 'yeah',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'zone'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set(unique_tokens)\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'son', 'empty', 'my', 'have', 'search', 'bangin', 'Will', 'air-speed', 'them', 'clop', 'do', 'climes', 'wants', '.', 'Supposing', 'back', 'get', 'south', 'maintain', '!', 'Arthur', 'zone', 'Saxons', 'strand', \"'re\", 'halves', 'guiding', 'horse', \"'\", 'ounce', 'Court', 'its', 'migrate', 'anyway', 'ARTHUR', 'bring', 'he', 'England', 'a', 'weight', 'Uther', 'coconut', 'may', 'Please', 'they', 'Not', 'No', 'held', 'from', 'speak', 'line', 'not', 'using', 'Camelot', 'or', 'join', \"'em\", 'But', '(', 'bird', 'simple', 'type', 'is', 'must', 'tell', 'this', 'since', 'maybe', 'does', 'plover', 'then', 'will', ',', 'be', 'under', 'Mercea', 'all', 'house', 'kingdom', ')', 'KING', 'husk', 'ratios', 'land', 'lord', 'coconuts', 'an', 'matter', 'ask', '?', 'covered', 'goes', 'in', 'at', 'scene_one', 'grips', 'court', 'pound', 'yet', 'if', 'and', 'Listen', 'Well', 'order', 'forty-three', 'Who', '[', '--', 'five', 'Patsy', 'Yes', 'these', 'who', 'go', 'swallows', 'SOLDIER', 'Whoa', 'That', 'yeah', 'European', '2', 'temperate', 'You', ':', 'your', 'wind', 'winter', 'could', 'creeper', ']', 'why', 'Are', 'What', 'beat', \"n't\", 'So', 'together', 'master', 'Halt', 'through', 'where', 'African', 'of', 'Oh', '...', 'minute', 'Where', 'breadth', 'it', 'servant', 'sun', 'right', 'on', 'I', 'course', 'the', 'sovereign', 'knights', 'tropical', 'Wait', 'We', 'wings', 'just', 'am', 'carried', 'got', 'second', 'castle', 'point', 'with', 'fly', 'defeator', 'The', 'by', 'agree', 'me', 'warmer', 'our', 'Pull', 'grip', 'but', 'two', 'times', 'Pendragon', 'length', '1', 'seek', 'suggesting', 'here', 'you', 'King', 'to', 'They', 'needs', 'found', 'A', 'one', \"'d\", 'velocity', 'carry', 'there', 'SCENE', 'Ridden', 'snows', 'non-migratory', 'every', 'dorsal', 'mean', 'swallow', \"'ve\", 'that', \"'m\", 'other', 'Found', 'carrying', 'strangers', 'interested', 'ridden', 'trusty', 'Am', 'It', \"'s\", 'martin', 'feathers', 'are', 'question', 'Britons', 'use', 'print', '#', 'In'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set(word_tokenize(scene_one)) # one line code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Tokenization is fundamental to NLP, and we'll end up using it a lot in text mining and information retrieval projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(580, 588), match='coconuts'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# This regular expression uses the . character, which matches any character (except a newline), \n",
    "# and the * character, which matches zero or more occurrences of the preceding character. \n",
    "# The .* pattern will match any character zero or more times, so it will match anything that appears within the square brackets.\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "\n",
    "# Create a pattern to match the script notation (e.g. Character:), assigning the result to pattern2. \n",
    "# Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
    "\n",
    "pattern2 = r\"[\\w\\s]+:\" # +: matches one or more of the preceding(previous) character or expression\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Now that we're familiar with the basics of tokenization and regular expressions, it's time to learn about more advanced tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tokenization with NLTK and regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brindha', 'has', '11', 'cats', 'and', '12', 'dogs']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "match_digits_and_words = ('(\\d+|\\w+)') # OR is represented using |; () - groups; [] - character ranges\n",
    "re.findall(match_digits_and_words, 'Brindha has 11 cats and 12 dogs!!') # notice, it does not match puntuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character range with re.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str_1 = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "my_str_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(my_str_1, r\"(\\w+|\\?|!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_str_1, r\"(\\w+|#\\d|\\?|!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_str_1, r\"(#\\d\\w+\\?!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_str_1, r\"\\s+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']\n",
    "print(tweets)\n",
    "print(type(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([#@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create an instance of TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# Tokenize each tweet into a new list all_tokens\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets] # list comprehension\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Isn't NLP fun?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing, ASCII (American Standard Code for Information Interchange) is a character encoding standard for electronic communication. It represents 128 characters, which include the upper and lowercase letters of the English alphabet, numbers, punctuation marks, and some special characters. ASCII code is used to represent text in computers, communications equipment, and other devices that work with text.\n",
    "\n",
    "Non-ASCII characters are characters that are not part of the ASCII character set. ASCII only includes 128 characters, while there are thousands of characters in various scripts and writing systems used around the world. For example, the ASCII character set does not include accented characters, such as á, è, or ñ, which are commonly used in Spanish, French, and other languages. To represent these characters, you need to use a character encoding that includes non-ASCII characters, such as Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German with emoji!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\n"
     ]
    }
   ],
   "source": [
    "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\"\n",
    "print(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only capital words\n",
    "\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words)) # (string, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPKElEQVR4nO3cb4xldX3H8fdHdo2Cmm3cad3uH4amxFRNlM1kgZIQ4p+Gf5G28QEklYQnWwk20Jo01Acan2nSmAYxbDZClZRiKCDZ6KKSFCs8AN1dlz/Larq1KCPbsmrddQsprv32wT1LJ5eZuXd27nBmf75fyc3cc87vnvNhMvvh3N8956aqkCSd/l7XdwBJ0mRY6JLUCAtdkhphoUtSIyx0SWrEmr4OvH79+pqenu7r8JJ0Wtq7d+9Pq2pqvm29Ffr09DR79uzp6/CSdFpK8qOFtjnlIkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhoxstCTvCHJd5I8keRAkk/NMyZJbklyKMmTSbauTFxJ0kLGuQ79f4D3VtXxJGuBR5M8WFWPzRlzGXBu9zgfuK37KUl6jYw8Q6+B493i2u4x/CXqVwF3dmMfA9Yl2TDZqJKkxYx1p2iSM4C9wO8Dn6+qx4eGbASem7M82607PLSf7cB2gC1btpxiZEmTNn3z13o79rOfvqK3Y7dmrA9Fq+rXVfUeYBOwLcm7hoZkvpfNs5+dVTVTVTNTU/N+FYEk6RQt6SqXqvoF8C3g0qFNs8DmOcubgOeXE0yStDTjXOUylWRd9/yNwPuB7w8N2wVc213tcgFwtKoOI0l6zYwzh74B+FI3j/464J6q+mqSjwBU1Q5gN3A5cAh4EbhuhfJKkhYwstCr6kngvHnW75jzvIAbJhtNkrQU3ikqSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY0YWehJNid5OMnBJAeS3DjPmEuSHE2yv3t8YmXiSpIWsmaMMSeAj1XVviRvBvYmeaiqnhka90hVXTn5iJKkcYw8Q6+qw1W1r3v+S+AgsHGlg0mSlmZJc+hJpoHzgMfn2XxhkieSPJjknQu8fnuSPUn2HDlyZOlpJUkLGrvQk7wJuA+4qaqODW3eB5xdVe8GPgc8MN8+qmpnVc1U1czU1NQpRpYkzWesQk+ylkGZ31VV9w9vr6pjVXW8e74bWJtk/USTSpIWNc5VLgFuBw5W1WcXGPO2bhxJtnX7/dkkg0qSFjfOVS4XAR8Gnkqyv1v3cWALQFXtAD4EXJ/kBPAScHVV1eTjSpIWMrLQq+pRICPG3ArcOqlQkqSl805RSWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjRhZ6Ek2J3k4ycEkB5LcOM+YJLklyaEkTybZujJxJUkLWTPGmBPAx6pqX5I3A3uTPFRVz8wZcxlwbvc4H7it+ylJeo2MPEOvqsNVta97/kvgILBxaNhVwJ018BiwLsmGiaeVJC1onDP0VySZBs4DHh/atBF4bs7ybLfu8NDrtwPbAbZs2bLEqNJrZ/rmr/Vy3Gc/fUUvx1Ubxv5QNMmbgPuAm6rq2PDmeV5Sr1pRtbOqZqpqZmpqamlJJUmLGqvQk6xlUOZ3VdX98wyZBTbPWd4EPL/8eJKkcY1zlUuA24GDVfXZBYbtAq7trna5ADhaVYcXGCtJWgHjzKFfBHwYeCrJ/m7dx4EtAFW1A9gNXA4cAl4Erpt4UknSokYWelU9yvxz5HPHFHDDpEJJkpbOO0UlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREjCz3JHUleSPL0AtsvSXI0yf7u8YnJx5QkjbJmjDFfBG4F7lxkzCNVdeVEEkmSTsnIM/Sq+jbw89cgiyRpGSY1h35hkieSPJjknQsNSrI9yZ4ke44cOTKhQ0uSYDKFvg84u6reDXwOeGChgVW1s6pmqmpmampqAoeWJJ207EKvqmNVdbx7vhtYm2T9spNJkpZk2YWe5G1J0j3f1u3zZ8vdryRpaUZe5ZLkbuASYH2SWeCTwFqAqtoBfAi4PskJ4CXg6qqqFUssSZrXyEKvqmtGbL+VwWWNkqQeeaeoJDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRows9CR3JHkhydMLbE+SW5IcSvJkkq2TjylJGmWcM/QvApcusv0y4NzusR24bfmxJElLNbLQq+rbwM8XGXIVcGcNPAasS7JhUgElSeNZM4F9bASem7M82607PDwwyXYGZ/Fs2bLllA84ffPXTvm1y/Xsp6/o7diSJqfFHpnEh6KZZ13NN7CqdlbVTFXNTE1NTeDQkqSTJlHos8DmOcubgOcnsF9J0hJMotB3Add2V7tcABytqldNt0iSVtbIOfQkdwOXAOuTzAKfBNYCVNUOYDdwOXAIeBG4bqXCSpIWNrLQq+qaEdsLuGFiiSRJp8Q7RSWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpEWMVepJLk/wgyaEkN8+z/ZIkR5Ps7x6fmHxUSdJi1owakOQM4PPAB4BZ4LtJdlXVM0NDH6mqK1cgoyRpDOOcoW8DDlXVD6vqZeDLwFUrG0uStFTjFPpG4Lk5y7PdumEXJnkiyYNJ3jnfjpJsT7InyZ4jR46cQlxJ0kLGKfTMs66GlvcBZ1fVu4HPAQ/Mt6Oq2llVM1U1MzU1taSgkqTFjVPos8DmOcubgOfnDqiqY1V1vHu+G1ibZP3EUkqSRhqn0L8LnJvknCSvB64Gds0dkORtSdI939bt92eTDitJWtjIq1yq6kSSjwLfAM4A7qiqA0k+0m3fAXwIuD7JCeAl4OqqGp6WkSStoJGFDq9Mo+weWrdjzvNbgVsnG02StBTeKSpJjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRYxV6kkuT/CDJoSQ3z7M9SW7ptj+ZZOvko0qSFjOy0JOcAXweuAx4B3BNkncMDbsMOLd7bAdum3BOSdII45yhbwMOVdUPq+pl4MvAVUNjrgLurIHHgHVJNkw4qyRpEWvGGLMReG7O8ixw/hhjNgKH5w5Ksp3BGTzA8SQ/WFLa/7ce+OkpvnZZ8plFN/eWawyrNZu55hjx9wUN/r7G+G9ejlX5+8pnlpXr7IU2jFPomWddncIYqmonsHOMYy4eKNlTVTPL3c+krdZcsHqzmWtpzLU0v2m5xplymQU2z1neBDx/CmMkSStonEL/LnBuknOSvB64Gtg1NGYXcG13tcsFwNGqOjy8I0nSyhk55VJVJ5J8FPgGcAZwR1UdSPKRbvsOYDdwOXAIeBG4buUiAxOYtlkhqzUXrN5s5loacy3Nb1SuVL1qqluSdBryTlFJaoSFLkmNOK0KPckdSV5I8nTfWeZKsjnJw0kOJjmQ5Ma+MwEkeUOS7yR5osv1qb4zzZXkjCTfS/LVvrOclOTZJE8l2Z9kT995TkqyLsm9Sb7f/Z1duAoyvb37PZ18HEtyU9+5AJL8Zfc3/3SSu5O8oe9MAElu7DIdWInf1Wk1h57kYuA4g7tS39V3npO6u2I3VNW+JG8G9gJ/XFXP9JwrwFlVdTzJWuBR4Mbubt7eJfkrYAZ4S1Vd2XceGBQ6MFNVq+pmlCRfAh6pqi90V5udWVW/6DnWK7qvCPkJcH5V/ajnLBsZ/K2/o6peSnIPsLuqvthzrncxuNN+G/Ay8HXg+qr610kd47Q6Q6+qbwM/7zvHsKo6XFX7uue/BA4yuFO2V91XMRzvFtd2j1Xxf/Akm4ArgC/0nWW1S/IW4GLgdoCqenk1lXnnfcC/9V3mc6wB3phkDXAmq+O+mD8AHquqF6vqBPAvwJ9M8gCnVaGfDpJMA+cBj/ccBXhlWmM/8ALwUFWtilzA3wF/DfxvzzmGFfDNJHu7r6pYDX4POAL8fTdF9YUkZ/UdasjVwN19hwCoqp8Afwv8mMHXjxytqm/2mwqAp4GLk7w1yZkMLvXePOI1S2KhT1CSNwH3ATdV1bG+8wBU1a+r6j0M7t7d1r3t61WSK4EXqmpv31nmcVFVbWXwDaI3dNN8fVsDbAVuq6rzgP8GXvU11n3ppoA+CPxT31kAkvwWgy8MPAf4XeCsJH/WbyqoqoPAZ4CHGEy3PAGcmOQxLPQJ6eao7wPuqqr7+84zrHuL/i3g0n6TAHAR8MFuvvrLwHuT/EO/kQaq6vnu5wvAVxjMd/ZtFpid8+7qXgYFv1pcBuyrqv/sO0jn/cC/V9WRqvoVcD/whz1nAqCqbq+qrVV1MYPp44nNn4OFPhHdh4+3Awer6rN95zkpyVSSdd3zNzL4Q/9+r6GAqvqbqtpUVdMM3qr/c1X1fgaV5KzuQ226KY0/YvA2uVdV9R/Ac0ne3q16H9DrB+5DrmGVTLd0fgxckOTM7t/m+xh8rtW7JL/d/dwC/CkT/r2N822Lq0aSu4FLgPVJZoFPVtXt/aYCBmecHwae6uarAT5eVbv7iwTABuBL3RUIrwPuqapVc4ngKvQ7wFcGHcAa4B+r6uv9RnrFXwB3ddMbP2Tlv15jLN1c8AeAP+87y0lV9XiSe4F9DKY0vsfq+QqA+5K8FfgVcENV/dckd35aXbYoSVqYUy6S1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXi/wBHggqgzzBPRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist([1,5,5,7,7,7,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not the prettiest chart by default, but making it look nicer is fairly easy with more arguments and several available helper libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'pretty', 'cool', 'tool', '!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 1, 6, 4, 4, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths = [len(w) for w in words] # list comprehension\n",
    "word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/brindhamanivannan/Desktop/KaggleX/DataCamp/scene_one.txt\", \"r\") as file:\n",
    "    holy_grail = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(holy_grail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
      "SOLDIER #1: What?  Ridden on a horse?\n",
      "ARTHUR: Yes!\n",
      "SOLDIER #1: You're using coconuts!\n",
      "ARTHUR: What?\n",
      "SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n",
      "ARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\n",
      "SOLDIER #1: Where'd you get the coconuts?\n",
      "ARTHUR: We found them.\n",
      "SOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\n",
      "ARTHUR: What do you mean?\n",
      "SOLDIER #1: Well, this is a temperate zone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(holy_grail[:998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " ' Whoa there!  [clop clop clop] ',\n",
       " ' Halt!  Who goes there?',\n",
       " ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " ' Pull the other one!']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all script lines for speaker\n",
    "prompt_pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "processed_lines = [re.sub(prompt_pattern, '', l) for l in lines]\n",
    "processed_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SCENE', '1', 'wind', 'clop', 'clop', 'clop'],\n",
       " ['KING', 'ARTHUR', 'Whoa', 'there', 'clop', 'clop', 'clop'],\n",
       " ['SOLDIER', '1', 'Halt', 'Who', 'goes', 'there'],\n",
       " ['ARTHUR',\n",
       "  'It',\n",
       "  'is',\n",
       "  'I',\n",
       "  'Arthur',\n",
       "  'son',\n",
       "  'of',\n",
       "  'Uther',\n",
       "  'Pendragon',\n",
       "  'from',\n",
       "  'the',\n",
       "  'castle',\n",
       "  'of',\n",
       "  'Camelot',\n",
       "  'King',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Britons',\n",
       "  'defeator',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Saxons',\n",
       "  'sovereign',\n",
       "  'of',\n",
       "  'all',\n",
       "  'England'],\n",
       " ['SOLDIER', '1', 'Pull', 'the', 'other', 'one']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "tokenized_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 7,\n",
       " 6,\n",
       " 26,\n",
       " 6,\n",
       " 41,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 16,\n",
       " 19,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 30,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 31,\n",
       " 22,\n",
       " 23,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 17,\n",
       " 8,\n",
       " 15,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 26,\n",
       " 6,\n",
       " 41,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 16,\n",
       " 19,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 30,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 31,\n",
       " 22,\n",
       " 23,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 17,\n",
       " 8,\n",
       " 15,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "line_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
